---
title: "Course Project"
author: "Claudio Vasconcelos Braga"
date: "1 de setembro de 2016"
output: html_document
---

#1)OBJECTIVE

Research on activity recognition has traditionally focus on discriminating between different activities, i.e. to predict "which" activity was performed at a specic point in time.
The quality of executing an activity, the "how (well)", has only received little attention so far, even though it potentially provides useful information for a large variety of applications.

In this work we build a model that predict the manner in which the dumbbell lifting is executed. We will also apply the model on a test base.


#2)COLLECTED DATA 

Six male participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions:
Class A - Exactly according to the specification
Class B - Throwing the elbows to the front
Class C - lifting the dumbbell only halfway
Class D - lowering the dumbbell only halfway
Class E - Throwing the hips to the front

The variables register the values of 4 sensors (glove, armband, lumbar belt and
dumbbell). For feature extraction we used a sliding window approach with different lengths from 0.5 second to 2.5 seconds, with 0.5 second overlap. In each step of the sliding window approach we calculated features on the Euler angles (roll, pitch
and yaw), as well as the raw accelerometer, gyroscope and magnetometer readings. For the Euler angles of each of the four sensors we calculated eight features: mean, variance, standard deviation, max, min, amplitude, kurtosis and skewness,
generating in total 96 derived feature sets.

As training data, We use the following database:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

```{r echo=FALSE}
library(lattice)
library(ggplot2)
library(caret)
#setwd("~/Business Intelligence/Aula Coursera Data Mining/8 - Machine Learning")
#setwd("e:/8 - Machine Learning")
setwd("~/DM/Pratical Machine Learning")
#setwd("~/R")

training<- read.csv("pml-training.csv",na.string=c("NA","#DIV/0!"))
testing<-read.csv("pml-testing.csv")
```

The database has `r dim(training)[1]` observation and `r dim(training)[2]` variables.


#3)DATA PREPARATION

Before built the model, we did the steps bellow:

3.1)Excluding variables with more than 30% of NA.

```{r echo=FALSE}
qtdNA<-apply(training,2,function(oo) sum(is.na(oo)))
training<-training[,qtdNA<(19622*0.3)]
testing<-testing[,qtdNA<(19622*0.3)]
```

Total:`r length(which(qtdNA>=(19622*0.3)))` variables excluded.

3.2)Excluding variables not important

```{r echo=FALSE}
excluded<-c(1,3,4,5,6,7)
NamesExcluded<-names(training)[excluded]
training<-training[,-excluded]
testing<-testing[,-excluded]
```

Excluded variables: `r NamesExcluded`

3.3) Excluding variables with correlation >0.9

Hightly correlated (>0.9):

```{r echo=FALSE}
corelation<-cor(training[,!(names(training)=="classe" | names(training)=="user_name")])
HighCor<-corelation>0.9
triag<-upper.tri(HighCor,diag=FALSE)
HighCor<-triag & HighCor #transforming in triangular

Pos<-which (HighCor)
lines<-ceiling(Pos/dim(HighCor))
columns<-Pos %% dim(HighCor)
Correlacionados<-cbind(rownames(HighCor)[lines],colnames(HighCor)[columns])

training<-training[,!(names(training) %in% 
                        c("roll_belt","total_accel_belt","gyros_dumbbell_z"))]
testing<-testing[,!(names(testing) %in% 
                      c("roll_belt","total_accel_belt","gyros_dumbbell_z"))]
Correlacionados
```

Excluded Variables: roll_belt, total_accel_belt and gyros_dumbbell_z.

3.4) Removal of outliers

```{r echo=FALSE}
IdentifyOutliers<-function (x,qi=0.05,qs=0.95,larg=1)
{
  qnt<-quantile(x,probs=c(qi,qs))
  H1<-larg*(qnt[2]-qnt[1])
  y<-which(   (x<(qnt[1]-H1)) | (x>(qnt[2]+H1)) )
  y
}

exc<-NULL
for (i in 2:(ncol(training)-1)){ # skip first column (user_name)
  exc<-c(exc,IdentifyOutliers(training[,i],qi=0.01,qs=0.99,larg=1))
}
training<-training[-exc, ]
```

We chose to remove observations with outliers. We consider outliers when the variable are bellow first percentile or above 99 percentile in amount greater than the difference between percentiles. `r length(exc)` observations were excluded.

#4) Data visualization

```{r echo=FALSE}
op <- par(oma=c(5,7,1,1))
par(mfrow=c(3, 6))
par(mar=c(2,2,2,2))
#par(mar = rep(2, 4))
colnames<- colnames (training)
for (i in 2:49) {
  hist(training[,i], main=colnames[i], probability=TRUE, col="gray", border="white")
  d <- density(training[,i])
  lines(d, col="red")
}

```

#5) Data Partition

```{r echo=FALSE}
in1<-createDataPartition(training$classe,p=0.80,list=FALSE)
Val<-training[-in1,]
sub2<-training[in1,]
in2<-createDataPartition(sub2$class,p=0.25,list=FALSE)
Test<-sub2[in2,]
Train<-sub2[-in2,]

tot<-dim(Train)[1] +dim(Test)[1]+ dim(Val)[1]
dim1<-dim(Train)[1]/tot*100
dim2<-dim(Test)[1]/tot*100
dim3<-dim(Val)[1]/tot*100
```

The data (`r tot` observations) were divided into:

Train `r dim1`%

Test  `r dim2`% 

Val   `r dim3`%

#6) Data Normalization

All numeric variables from Train, Test and Validation were normalized considering the Train dataset.

```{r echo=FALSE}
VarCategoricas<-which(names(Train)=="classe" |names(Train)=="user_name" )
preObj<-preProcess(Train[,-VarCategoricas],method=c("center","scale"))
Train<-predict(preObj,Train)
Test <-predict(preObj,Test)
Val  <-predict(preObj,Val)
testing<-predict(preObj,testing)
```

#7) Modelling

We used Random Forest with bootstrapping as crosvalidation.

```{r echo=FALSE, cache=TRUE}
Mod1<-train (classe~.,data=Train,method="rf")

ResultPrev1<-predict(Mod1,Test)
conf<-confusionMatrix(Test$classe,ResultPrev1)
#9957% accuracy

```

We used the test set to measure the accuracy of the model.

Confusion Matrix:
```{r echo=FALSE, cache=TRUE}
conf$table
```

Accuracy: `r conf$overall[1]`. 


This show that the model archive a good precision. The out of sample error is low.

#8) Quiz4

We used the model to predict 20 different test cases.

The result is:
```{r echo=FALSE}
ResultQuiz4<-predict(Mod1,testing)
ResultQuiz4
```
